{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6debe259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession dan library MLlib siap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 61921)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\serializers.py\", line 595, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Impor library MLlib\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "# Hentikan SparkSession jika ada yang aktif\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Buat SparkSession baru\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnModeling\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession dan library MLlib siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb714f",
   "metadata": {},
   "source": [
    "# Load Data & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f56dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection selesai. Skema akhir untuk model:\n",
      "root\n",
      " |-- is_churn: integer (nullable = true)\n",
      " |-- city: integer (nullable = true)\n",
      " |-- age_group: string (nullable = true)\n",
      " |-- total_transactions: long (nullable = true)\n",
      " |-- total_payment_plan_days: long (nullable = true)\n",
      " |-- avg_discount: double (nullable = true)\n",
      " |-- count_cancel: long (nullable = true)\n",
      " |-- days_since_last_activity: integer (nullable = true)\n",
      " |-- total_secs_last_30d: double (nullable = true)\n",
      " |-- active_days_last_30d: long (nullable = true)\n",
      " |-- activity_ratio_secs: double (nullable = true)\n",
      " |-- percent_complete_last_30d: double (nullable = true)\n",
      " |-- lifetime_unq_songs: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Muat data master_feature_table_3.parquet\n",
    "data_path = \"data/master_feature_table_3.parquet\"\n",
    "df = spark.read.parquet(data_path)\n",
    "df.cache()\n",
    "\n",
    "# 2. Daftar Fitur yang DIBUANG (Berdasarkan EDA & Korelasi)\n",
    "cols_to_drop = [\n",
    "    \"msno\",                     # ID\n",
    "    \"last_transaction_date\",    # Format tanggal\n",
    "    \"last_expiry_date\",         # Format tanggal\n",
    "    \n",
    "    # --- Berdasarkan Temuan EDA Correlation ---\n",
    "    # Dibuang karena Redundant (Korelasi > 0.85)\n",
    "    \"count_auto_renew\",\n",
    "    #\"total_transactions\",       # Korelasi 0.91 dg count_auto_renew\n",
    "    #\"total_payment_plan_days\",  # Korelasi 0.88 dg total_transactions\n",
    "    \"total_secs_last_90d\",      # Korelasi 0.94 dg total_secs_last_30d\n",
    "    \"active_days_last_90d\",     # Korelasi 0.94 dg active_days_last_30d\n",
    "    \n",
    "    # Dibuang karena Tidak Prediktif (Berdasarkan EDA)\n",
    "    \"membership_duration_days\",\n",
    "    \"registered_via\",\n",
    "    #\"city\"\n",
    "    \"lifetime_active_days\",\n",
    "    \"lifetime_uni_songs\",\n",
    "    \n",
    "]\n",
    "\n",
    "# 3. Terapkan Feature Selection\n",
    "df_selected = df.drop(*cols_to_drop)\n",
    "\n",
    "print(\"Feature selection selesai. Skema akhir untuk model:\")\n",
    "df_selected.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589088e",
   "metadata": {},
   "source": [
    "# Define Features Type & Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur Kategorikal: ['age_group', 'city']\n",
      "Fitur Numerik: ['total_transactions', 'total_payment_plan_days', 'avg_discount', 'count_cancel', 'days_since_last_activity', 'total_secs_last_30d', 'active_days_last_30d', 'activity_ratio_secs', 'percent_complete_last_30d', 'lifetime_unq_songs']\n"
     ]
    }
   ],
   "source": [
    "# define tipe fitur& pipeline preprocessing\n",
    "# 1. Tentukan fitur kategorikal dan numerik (dari sisa kolom)\n",
    "#categorical_cols = [\"age_group\", \"city\", \"registered_via\"]\n",
    "categorical_cols = [\"age_group\", \"city\"]\n",
    "#categorical_cols = [\"age_group\"]\n",
    "\n",
    "# Semua kolom lain selain 'is_churn' dan kategorikal adalah numerik\n",
    "numerical_cols = [\n",
    "    col for col in df_selected.columns \n",
    "    if col not in categorical_cols + [\"is_churn\"]\n",
    "]\n",
    "\n",
    "print(f\"Fitur Kategorikal: {categorical_cols}\")\n",
    "print(f\"Fitur Numerik: {numerical_cols}\")\n",
    "\n",
    "# --- TAHAPAN PIPELINE PREPROCESSING ---\n",
    "\n",
    "# Tahap 1: StringIndexer (Hanya untuk 'age_group' karena 'city' & 'registered_via' sudah angka)\n",
    "# Kita perlu mengubah \"Unknown\", \"18-25\" menjadi 0.0, 1.0, dst.\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"age_group\", \n",
    "    outputCol=\"age_group_idx\", \n",
    "    handleInvalid=\"keep\" # Mengubah null/unknown menjadi indeks khusus\n",
    ")\n",
    "\n",
    "# Tahap 2: OneHotEncoder (Untuk SEMUA kategorikal)\n",
    "# Mengubah [0.0, 1.0, 2.0] menjadi vector [1,0,0], [0,1,0], [0,0,1]\n",
    "encoder = OneHotEncoder(\n",
    "    #inputCols=[\"age_group_idx\", \"city\", \"registered_via\"],\n",
    "    inputCols=[\"age_group_idx\", \"city\"],\n",
    "    #outputCols=[\"age_group_vec\", \"city_vec\", \"registered_via_vec\"]\n",
    "    outputCols=[\"age_group_vec\" , \"city_vec\"]\n",
    ")\n",
    "\n",
    "# Tahap 3: VectorAssembler (Hanya untuk fitur NUMERIK)\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=numerical_cols, \n",
    "    outputCol=\"numerical_features\"\n",
    ")\n",
    "\n",
    "# Tahap 4: StandardScaler (Untuk fitur numerik)\n",
    "# Menyamakan skala semua fitur numerik (penting untuk Logistic Regression)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numerical_features\", \n",
    "    outputCol=\"scaled_numerical_features\"\n",
    ")\n",
    "\n",
    "# Tahap 5: VectorAssembler Final (Menggabungkan SEMUA fitur)\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"age_group_vec\", \n",
    "        \"city_vec\", \n",
    "        #\"registered_via_vec\", \n",
    "        \"scaled_numerical_features\"\n",
    "    ],\n",
    "    outputCol=\"features\" # Ini adalah kolom akhir yang dibutuhkan model\n",
    ")\n",
    "\n",
    "# Gabungkan semua tahapan preprocessing menjadi satu pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        indexer, \n",
    "        #encoder, \n",
    "        assembler_num, \n",
    "        scaler, \n",
    "        assembler_final\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0905791",
   "metadata": {},
   "source": [
    "# Data Splitting & Oversampling (imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db345207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membagi data menjadi 80% Latih, 20% Uji...\n",
      "Baris data Latih (sebelum oversampling): 865,704\n",
      "Baris data Uji: 216,486\n",
      "Melakukan oversampling pada data latih...\n",
      "Rasio (0:1): 9.93 : 1\n",
      "Baris data Latih (setelah oversampling): 1,572,027\n",
      "Verifikasi data latih baru:\n",
      "+--------+------+\n",
      "|is_churn| count|\n",
      "+--------+------+\n",
      "|       0|786515|\n",
      "|       1|785512|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Split & Oversampling (imbalance)\n",
    "# 1. Bagi data menjadi set Latihan (80%) dan Uji (20%)\n",
    "# stratifikasi berdasarkan 'is_churn' agar proporsinya sama\n",
    "print(\"Membagi data menjadi 80% Latih, 20% Uji...\")\n",
    "train_data, test_data = df_selected.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "\n",
    "print(f\"Baris data Latih (sebelum oversampling): {train_data.count():,}\")\n",
    "print(f\"Baris data Uji: {test_data.count():,}\")\n",
    "\n",
    "# 2. Lakukan Oversampling pada Data Latih (HANYA PADA TRAIN_DATA)\n",
    "print(\"Melakukan oversampling pada data latih...\")\n",
    "\n",
    "# Hitung rasio imbalance\n",
    "count_class_0 = train_data.filter(col(\"is_churn\") == 0).count()\n",
    "count_class_1 = train_data.filter(col(\"is_churn\") == 1).count()\n",
    "ratio = count_class_0 / count_class_1\n",
    "\n",
    "print(f\"Rasio (0:1): {ratio:.2f} : 1\")\n",
    "\n",
    "# Pisahkan kelas\n",
    "df_majority = train_data.filter(col(\"is_churn\") == 0)\n",
    "df_minority = train_data.filter(col(\"is_churn\") == 1)\n",
    "\n",
    "# Lakukan oversampling (duplikasi acak) pada kelas minoritas\n",
    "df_minority_oversampled = df_minority.sample(\n",
    "    withReplacement=True, \n",
    "    fraction=ratio, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Gabungkan kembali menjadi data latih yang seimbang\n",
    "train_data_oversampled = df_majority.unionAll(df_minority_oversampled)\n",
    "\n",
    "print(f\"Baris data Latih (setelah oversampling): {train_data_oversampled.count():,}\")\n",
    "print(\"Verifikasi data latih baru:\")\n",
    "train_data_oversampled.groupBy(\"is_churn\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371a4",
   "metadata": {},
   "source": [
    "# Define Model & Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fda1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melatih Logistic Regression...\n",
      "Melatih Random Forest...\n",
      "Melatih GBT...\n",
      "Semua model selesai dilatih.\n"
     ]
    }
   ],
   "source": [
    "# define model & train pipeline\n",
    "# 1. Definisikan 3 model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_churn\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_churn\", seed=42)\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"is_churn\", seed=42)\n",
    "\n",
    "# 2. Buat pipeline lengkap (Preprocessing + Model)\n",
    "pipeline_lr = Pipeline(stages=[preprocessing_pipeline, lr])\n",
    "pipeline_rf = Pipeline(stages=[preprocessing_pipeline, rf])\n",
    "pipeline_gbt = Pipeline(stages=[preprocessing_pipeline, gbt])\n",
    "\n",
    "# 3. Latih model\n",
    "# Model dilatih pada data latih yang sudah SEIMBANG (Oversampled)\n",
    "print(\"Melatih Logistic Regression...\")\n",
    "model_lr = pipeline_lr.fit(train_data_oversampled)\n",
    "\n",
    "print(\"Melatih Random Forest...\")\n",
    "model_rf = pipeline_rf.fit(train_data_oversampled)\n",
    "\n",
    "print(\"Melatih GBT...\")\n",
    "model_gbt = pipeline_gbt.fit(train_data_oversampled)\n",
    "\n",
    "print(\"Semua model selesai dilatih.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb06e7",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "206145e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membuat prediksi pada data uji (unseen & imbalanced)...\n",
      "\n",
      "--- Hasil Evaluasi Model ---\n",
      "\n",
      "Logistic Regression:\n",
      "  AUC-ROC: 0.8913\n",
      "  AUC-PR (Fokus Churn): 0.5138\n",
      "\n",
      "Random Forest:\n",
      "  AUC-ROC: 0.9262\n",
      "  AUC-PR (Fokus Churn): 0.6833\n",
      "\n",
      "GBT Classifier:\n",
      "  AUC-ROC: 0.9523\n",
      "  AUC-PR (Fokus Churn): 0.7344\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model pada Data Test (evaluasi balik ke data yg ga seimbang u/ lihat performa nyata model)\n",
    "# 1. Buat prediksi pada data UJI (yang tidak seimbang)\n",
    "print(\"Membuat prediksi pada data uji (unseen & imbalanced)...\")\n",
    "pred_lr = model_lr.transform(test_data)\n",
    "pred_rf = model_rf.transform(test_data)\n",
    "pred_gbt = model_gbt.transform(test_data)\n",
    "\n",
    "# 2. Definisikan Evaluator\n",
    "# menggunakan dua metrik utama untuk data imbalance:\n",
    "# AUC-ROC: Baik untuk mengukur performa keseluruhan\n",
    "# AUC-PR: (AreaUnderPrecisionRecall) Sangat baik untuk kelas minoritas yang langka\n",
    "\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_churn\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_churn\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "# 3. Hitung dan Tampilkan Hasil\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Hasil Evaluasi Model ---\")\n",
    "\n",
    "# Logistic Regression\n",
    "auc_roc_lr = evaluator_roc.evaluate(pred_lr)\n",
    "auc_pr_lr = evaluator_pr.evaluate(pred_lr)\n",
    "results['Logistic Regression'] = {'AUC-ROC': auc_roc_lr, 'AUC-PR': auc_pr_lr}\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  AUC-ROC: {auc_roc_lr:.4f}\")\n",
    "print(f\"  AUC-PR (Fokus Churn): {auc_pr_lr:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "auc_roc_rf = evaluator_roc.evaluate(pred_rf)\n",
    "auc_pr_rf = evaluator_pr.evaluate(pred_rf)\n",
    "results['Random Forest'] = {'AUC-ROC': auc_roc_rf, 'AUC-PR': auc_pr_rf}\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  AUC-ROC: {auc_roc_rf:.4f}\")\n",
    "print(f\"  AUC-PR (Fokus Churn): {auc_pr_rf:.4f}\")\n",
    "\n",
    "# GBT\n",
    "auc_roc_gbt = evaluator_roc.evaluate(pred_gbt)\n",
    "auc_pr_gbt = evaluator_pr.evaluate(pred_gbt)\n",
    "results['GBT'] = {'AUC-ROC': auc_roc_gbt, 'AUC-PR': auc_pr_gbt}\n",
    "print(f\"\\nGBT Classifier:\")\n",
    "print(f\"  AUC-ROC: {auc_roc_gbt:.4f}\")\n",
    "print(f\"  AUC-PR (Fokus Churn): {auc_pr_gbt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffea7e",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46aa22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion Matrix untuk: GBT Classifier ---\n",
      "[[179320.  17327.]\n",
      " [  2746.  17093.]]\n",
      "  Overall Accuracy:   0.9073\n",
      "  Recall (Churn=1):    0.8616\n",
      "  Precision (Churn=1): 0.4966\n",
      "  F1-Score (Churn=1):  0.6301\n",
      "\n",
      "--- Confusion Matrix untuk: Logistic Regression ---\n",
      "[[162220.  34427.]\n",
      " [  3783.  16056.]]\n",
      "  Overall Accuracy:   0.8235\n",
      "  Recall (Churn=1):    0.8093\n",
      "  Precision (Churn=1): 0.3180\n",
      "  F1-Score (Churn=1):  0.4566\n",
      "\n",
      "--- Confusion Matrix untuk: Random Forest ---\n",
      "[[170835.  25812.]\n",
      " [  3242.  16597.]]\n",
      "  Overall Accuracy:   0.8658\n",
      "  Recall (Churn=1):    0.8366\n",
      "  Precision (Churn=1): 0.3914\n",
      "  F1-Score (Churn=1):  0.5333\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def print_confusion_matrix(predictions, model_name):\n",
    "    # Mengubah prediksi menjadi RDD untuk MulticlassMetrics\n",
    "    preds_and_labels = predictions.select(\"prediction\", \"is_churn\").rdd.map(\n",
    "        lambda r: (float(r.prediction), float(r.is_churn))\n",
    "    )\n",
    "    \n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    print(f\"\\n--- Confusion Matrix untuk: {model_name} ---\")\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    Overall_Accuracy = metrics.accuracy\n",
    "    print(f\"  Overall Accuracy:   {Overall_Accuracy:.4f}\")\n",
    "\n",
    "    # TN, FP\n",
    "    # FN, TP\n",
    "    TN = confusion_matrix[0][0]\n",
    "    FP = confusion_matrix[0][1]\n",
    "    FN = confusion_matrix[1][0]\n",
    "    TP = confusion_matrix[1][1]\n",
    "    \n",
    "    Recall_Churn = TP / (TP + FN)\n",
    "    Precision_Churn = TP / (TP + FP)\n",
    "    F1_Churn = 2 * (Precision_Churn * Recall_Churn) / (Precision_Churn + Recall_Churn)\n",
    "    \n",
    "    print(f\"  Recall (Churn=1):    {Recall_Churn:.4f}\")\n",
    "    print(f\"  Precision (Churn=1): {Precision_Churn:.4f}\")\n",
    "    print(f\"  F1-Score (Churn=1):  {F1_Churn:.4f}\")\n",
    "\n",
    "# Jalankan untuk model terbaik (misal, GBT)\n",
    "print_confusion_matrix(pred_gbt, \"GBT Classifier\")\n",
    "\n",
    "# Jalankan untuk Logistic Regression\n",
    "print_confusion_matrix(pred_lr, \"Logistic Regression\")\n",
    "\n",
    "# Jalankan untuk Random Forest\n",
    "print_confusion_matrix(pred_rf, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5749b56",
   "metadata": {},
   "source": [
    "# Punya peli masih an cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71c023",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m results_cv = {}    \u001b[38;5;66;03m# Hasil dari cross_validate\u001b[39;00m\n\u001b[32m     25\u001b[39m results_split = {} \u001b[38;5;66;03m# Hasil dari single fit/predict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m.items():\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Melatih dan Menguji: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m     full_pipeline = Pipeline([\n\u001b[32m     31\u001b[39m         (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     32\u001b[39m         (\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, model)\n\u001b[32m     33\u001b[39m     ])\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate \n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, f1_score\n",
    "import numpy as np \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tentukan strategi Cross-Validation (disarankan K=5 atau K=10)\n",
    "# StratifiedKFold WAJIB karena data churn tidak seimbang\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Definisikan metrik yang akan diukur (fokus pada kelas '1'/churn)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy', \n",
    "    # Metrik untuk CHURN (Kelas 1)\n",
    "    'recall_churn': make_scorer(recall_score, pos_label=1),\n",
    "    'precision_churn': make_scorer(precision_score, pos_label=1),\n",
    "    'f1_churn': make_scorer(f1_score, pos_label=1),\n",
    "\n",
    "    # Metrik untuk TIDAK CHURN (Kelas 0) \n",
    "    'recall_non_churn': make_scorer(recall_score, pos_label=0),\n",
    "    'precision_non_churn': make_scorer(precision_score, pos_label=0),\n",
    "    'f1_non_churn': make_scorer(f1_score, pos_label=0)\n",
    "}\n",
    "\n",
    "results_cv = {}    # Hasil dari cross_validate\n",
    "results_split = {} # Hasil dari single fit/predict\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Melatih dan Menguji: {model_name} ---\")\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # =======================================================\n",
    "    # BAGIAN 1: SINGLE-SPLIT (Dibutuhkan untuk melihat report_dict lengkap)\n",
    "    # =======================================================\n",
    "    full_pipeline.fit(X_train, y_train) # Latih model untuk single-split\n",
    "    y_pred = full_pipeline.predict(X_test)\n",
    "    \n",
    "    # Simpan hasil single-split (untuk visualisasi report lengkap)\n",
    "    results_split[model_name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # =======================================================\n",
    "    # BAGIAN 2: CROSS-VALIDATION (Untuk mendapatkan skor paling andal)\n",
    "    # =======================================================\n",
    "    cv_scores = cross_validate(\n",
    "        full_pipeline, \n",
    "        X, y, # Menggunakan SEMUA data sampel\n",
    "        cv=cv, \n",
    "        scoring=scoring, \n",
    "        return_train_score=False, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Rata-ratakan skor dari 5 fold\n",
    "    # Pastikan 'accuracy' sudah ditambahkan ke dictionary 'scoring' Anda\n",
    "    avg_scores = {\n",
    "        'avg_accuracy': np.mean(cv_scores['test_accuracy']),\n",
    "        \n",
    "        # CHURN (Kelas 1)\n",
    "        'avg_recall_churn': np.mean(cv_scores['test_recall_churn']),\n",
    "        'avg_precision_churn': np.mean(cv_scores['test_precision_churn']),\n",
    "        'avg_f1_churn': np.mean(cv_scores['test_f1_churn']),\n",
    "\n",
    "        # TIDAK CHURN (Kelas 0) \n",
    "        'avg_recall_non_churn': np.mean(cv_scores['test_recall_non_churn']),\n",
    "        'avg_precision_non_churn': np.mean(cv_scores['test_precision_non_churn']),\n",
    "        'avg_f1_non_churn': np.mean(cv_scores['test_f1_non_churn']),\n",
    "    }\n",
    "\n",
    "    results_cv[model_name] = avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for model_name in models.keys():\n",
    "    report_split = results_split.get(model_name)\n",
    "    report_cv = results_cv.get(model_name)\n",
    "    \n",
    "    if not report_split or not report_cv:\n",
    "        print(f\"\\n--- Model: {model_name} (Hasil tidak lengkap) ---\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Model: **{model_name}** ---\")\n",
    "    # --- 1. Hasil dari Single Split (fit/predict) ---\n",
    "    print(\"  HASIL SPLIT TUNGGAL (80/20):\")\n",
    "    print(f\"    Akurasi Keseluruhan: {report_split['accuracy']:.4f}\")\n",
    "    \n",
    "    if '1' in report_split:\n",
    "        print(\"\\n     CHURN (Kelas 1):\")\n",
    "        print(f\"      Recall:    {report_split['1']['recall']:.4f}\")\n",
    "        print(f\"      Precision: {report_split['1']['precision']:.4f}\")\n",
    "        print(f\"      F1-Score:  {report_split['1']['f1-score']:.4f}\")\n",
    "    if '0' in report_split:\n",
    "        print(\"\\n     TIDAK CHURN (Kelas 0):\")\n",
    "        print(f\"      Recall: {report_split['0']['recall']:.4f}\")\n",
    "        print(f\"      Precision: {report_split['0']['precision']:.4f}\")\n",
    "        print(f\"      F1-Score: {report_split['0']['f1-score']:.4f}\")\n",
    "        \n",
    "    # --- 2. Hasil dari Cross-Validation (Rata-rata) ---\n",
    "    print(\"\\n  HASIL CROSS-VALIDATION (Rata-rata 5-Fold):\")\n",
    "    print(f\"    Akurasi Keseluruhan: {report_cv['avg_accuracy']:.4f}\")\n",
    "    \n",
    "    # Hasil Rata-rata CHURN (Kelas 1)\n",
    "    print(\"\\n     CHURN (Kelas 1):\")\n",
    "    print(f\"      Recall (Churn=1):    {report_cv['avg_recall_churn']:.4f}\")\n",
    "    print(f\"      Precision (Churn=1): {report_cv['avg_precision_churn']:.4f}\")\n",
    "    print(f\"      F1-Score (Churn=1):  {report_cv['avg_f1_churn']:.4f}\")\n",
    "    \n",
    "    # Hasil Rata-rata NON-CHURN (Kelas 0) <--- TAMBAHAN BARU\n",
    "    print(\"\\n     TIDAK CHURN (Kelas 0):\")\n",
    "    print(f\"      Recall (Non-Churn=0):    {report_cv['avg_recall_non_churn']:.4f}\")\n",
    "    print(f\"      Precision (Non-Churn=0): {report_cv['avg_precision_non_churn']:.4f}\")\n",
    "    print(f\"      F1-Score (Non-Churn=0):  {report_cv['avg_f1_non_churn']:.4f}\")\n",
    "    \n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f853981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ringkasan DataFrame Fokus Menggunakan CV ---\")\n",
    "comparison_data = []\n",
    "for model_name in models.keys():\n",
    "    \n",
    "    # Pengecekan data karena DataFrame akan gagal jika KeyError muncul\n",
    "    if model_name not in results_cv or 'avg_recall_non_churn' not in results_cv[model_name]:\n",
    "        continue\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Recall_CV (Churn=1)\": results_cv[model_name]['avg_recall_churn'],\n",
    "        \"Precision_CV (Churn=1)\": results_cv[model_name]['avg_precision_churn'],\n",
    "        \"Recall_CV (Non-Churn=0)\": results_cv[model_name]['avg_recall_non_churn'],\n",
    "        \"Akurasi_CV\": results_cv[model_name]['avg_accuracy'],\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sortir berdasarkan metrik yang paling penting (Recall Churn)\n",
    "print(comparison_df.sort_values(by=\"Recall_CV (Churn=1)\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
