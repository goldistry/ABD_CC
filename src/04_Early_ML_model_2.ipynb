{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6debe259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession dan library MLlib siap.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Impor library MLlib\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "# Hentikan SparkSession jika ada yang aktif\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Buat SparkSession baru\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnModeling\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession dan library MLlib siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb714f",
   "metadata": {},
   "source": [
    "# Load Data & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection selesai. Skema akhir untuk model:\n",
      "root\n",
      " |-- is_churn: integer (nullable = true)\n",
      " |-- city: integer (nullable = true)\n",
      " |-- age_group: string (nullable = true)\n",
      " |-- registered_via: integer (nullable = true)\n",
      " |-- avg_discount: double (nullable = true)\n",
      " |-- count_auto_renew: long (nullable = true)\n",
      " |-- count_cancel: long (nullable = true)\n",
      " |-- days_since_last_activity: integer (nullable = true)\n",
      " |-- total_secs_last_30d: double (nullable = true)\n",
      " |-- active_days_last_30d: long (nullable = true)\n",
      " |-- activity_ratio_secs: double (nullable = true)\n",
      " |-- percent_complete_last_30d: double (nullable = true)\n",
      " |-- lifetime_unq_songs: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Muat data master_feature_table_3.parquet\n",
    "data_path = \"data/master_feature_table_3.parquet\"\n",
    "df = spark.read.parquet(data_path)\n",
    "df.cache()\n",
    "\n",
    "# 2. Daftar Fitur yang DIBUANG (Berdasarkan EDA & Korelasi)\n",
    "cols_to_drop = [\n",
    "    \"msno\",                     # ID\n",
    "    \"last_transaction_date\",    # Format tanggal\n",
    "    \"last_expiry_date\",         # Format tanggal\n",
    "    \n",
    "    # --- Berdasarkan Temuan EDA Correlation ---\n",
    "    # Dibuang karena Redundant (Korelasi > 0.85)\n",
    "    \"total_transactions\",       # Korelasi 0.91 dg count_auto_renew\n",
    "    \"total_payment_plan_days\",  # Korelasi 0.88 dg total_transactions\n",
    "    \"total_secs_last_90d\",      # Korelasi 0.94 dg total_secs_last_30d\n",
    "    \"active_days_last_90d\",     # Korelasi 0.94 dg active_days_last_30d\n",
    "    \n",
    "    # Dibuang karena Tidak Prediktif (Berdasarkan EDA)\n",
    "    \"membership_duration_days\",\n",
    "    \"lifetime_active_days\"\n",
    "]\n",
    "\n",
    "# 3. Terapkan Feature Selection\n",
    "df_selected = df.drop(*cols_to_drop)\n",
    "\n",
    "print(\"Feature selection selesai. Skema akhir untuk model:\")\n",
    "df_selected.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589088e",
   "metadata": {},
   "source": [
    "# Define Features Type & Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ae631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur Kategorikal: ['age_group', 'city', 'registered_via']\n",
      "Fitur Numerik: ['avg_discount', 'count_auto_renew', 'count_cancel', 'days_since_last_activity', 'total_secs_last_30d', 'active_days_last_30d', 'activity_ratio_secs', 'percent_complete_last_30d', 'lifetime_unq_songs']\n"
     ]
    }
   ],
   "source": [
    "# define tipe fitur& pipeline preprocessing\n",
    "# 1. Tentukan fitur kategorikal dan numerik (dari sisa kolom)\n",
    "categorical_cols = [\"age_group\", \"city\", \"registered_via\"]\n",
    "\n",
    "# Semua kolom lain selain 'is_churn' dan kategorikal adalah numerik\n",
    "numerical_cols = [\n",
    "    col for col in df_selected.columns \n",
    "    if col not in categorical_cols + [\"is_churn\"]\n",
    "]\n",
    "\n",
    "print(f\"Fitur Kategorikal: {categorical_cols}\")\n",
    "print(f\"Fitur Numerik: {numerical_cols}\")\n",
    "\n",
    "# --- TAHAPAN PIPELINE PREPROCESSING ---\n",
    "\n",
    "# Tahap 1: StringIndexer (Hanya untuk 'age_group' karena 'city' & 'registered_via' sudah angka)\n",
    "# Kita perlu mengubah \"Unknown\", \"18-25\" menjadi 0.0, 1.0, dst.\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"age_group\", \n",
    "    outputCol=\"age_group_idx\", \n",
    "    handleInvalid=\"keep\" # Mengubah null/unknown menjadi indeks khusus\n",
    ")\n",
    "\n",
    "# Tahap 2: OneHotEncoder (Untuk SEMUA kategorikal)\n",
    "# Mengubah [0.0, 1.0, 2.0] menjadi vector [1,0,0], [0,1,0], [0,0,1]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"age_group_idx\", \"city\", \"registered_via\"],\n",
    "    outputCols=[\"age_group_vec\", \"city_vec\", \"registered_via_vec\"]\n",
    ")\n",
    "\n",
    "# Tahap 3: VectorAssembler (Hanya untuk fitur NUMERIK)\n",
    "assembler_num = VectorAssembler(\n",
    "    inputCols=numerical_cols, \n",
    "    outputCol=\"numerical_features\"\n",
    ")\n",
    "\n",
    "# Tahap 4: StandardScaler (Untuk fitur numerik)\n",
    "# Menyamakan skala semua fitur numerik (penting untuk Logistic Regression)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numerical_features\", \n",
    "    outputCol=\"scaled_numerical_features\"\n",
    ")\n",
    "\n",
    "# Tahap 5: VectorAssembler Final (Menggabungkan SEMUA fitur)\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"age_group_vec\", \n",
    "        \"city_vec\", \n",
    "        \"registered_via_vec\", \n",
    "        \"scaled_numerical_features\"\n",
    "    ],\n",
    "    outputCol=\"features\" # Ini adalah kolom akhir yang dibutuhkan model\n",
    ")\n",
    "\n",
    "# Gabungkan semua tahapan preprocessing menjadi satu pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        indexer, \n",
    "        encoder, \n",
    "        assembler_num, \n",
    "        scaler, \n",
    "        assembler_final\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0905791",
   "metadata": {},
   "source": [
    "# Data Splitting & Oversampling (imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db345207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membagi data menjadi 80% Latih, 20% Uji...\n",
      "Baris data Latih (sebelum oversampling): 865,704\n",
      "Baris data Uji: 216,486\n",
      "Melakukan oversampling pada data latih...\n",
      "Rasio (0:1): 9.93 : 1\n",
      "Baris data Latih (setelah oversampling): 1,572,027\n",
      "Verifikasi data latih baru:\n",
      "+--------+------+\n",
      "|is_churn| count|\n",
      "+--------+------+\n",
      "|       0|786515|\n",
      "|       1|785512|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Split & Oversampling (imbalance)\n",
    "# 1. Bagi data menjadi set Latihan (80%) dan Uji (20%)\n",
    "# stratifikasi berdasarkan 'is_churn' agar proporsinya sama\n",
    "print(\"Membagi data menjadi 80% Latih, 20% Uji...\")\n",
    "train_data, test_data = df_selected.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Baris data Latih (sebelum oversampling): {train_data.count():,}\")\n",
    "print(f\"Baris data Uji: {test_data.count():,}\")\n",
    "\n",
    "# 2. Lakukan Oversampling pada Data Latih (HANYA PADA TRAIN_DATA)\n",
    "print(\"Melakukan oversampling pada data latih...\")\n",
    "\n",
    "# Hitung rasio imbalance\n",
    "count_class_0 = train_data.filter(col(\"is_churn\") == 0).count()\n",
    "count_class_1 = train_data.filter(col(\"is_churn\") == 1).count()\n",
    "ratio = count_class_0 / count_class_1\n",
    "\n",
    "print(f\"Rasio (0:1): {ratio:.2f} : 1\")\n",
    "\n",
    "# Pisahkan kelas\n",
    "df_majority = train_data.filter(col(\"is_churn\") == 0)\n",
    "df_minority = train_data.filter(col(\"is_churn\") == 1)\n",
    "\n",
    "# Lakukan oversampling (duplikasi acak) pada kelas minoritas\n",
    "df_minority_oversampled = df_minority.sample(\n",
    "    withReplacement=True, \n",
    "    fraction=ratio, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Gabungkan kembali menjadi data latih yang seimbang\n",
    "train_data_oversampled = df_majority.unionAll(df_minority_oversampled)\n",
    "\n",
    "print(f\"Baris data Latih (setelah oversampling): {train_data_oversampled.count():,}\")\n",
    "print(\"Verifikasi data latih baru:\")\n",
    "train_data_oversampled.groupBy(\"is_churn\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371a4",
   "metadata": {},
   "source": [
    "# Define Model & Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fda1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melatih Logistic Regression...\n",
      "Melatih Random Forest...\n",
      "Melatih GBT...\n",
      "Semua model selesai dilatih.\n"
     ]
    }
   ],
   "source": [
    "# define model & train pipeline\n",
    "# 1. Definisikan 3 model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_churn\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_churn\", seed=42)\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"is_churn\", seed=42)\n",
    "\n",
    "# 2. Buat pipeline lengkap (Preprocessing + Model)\n",
    "pipeline_lr = Pipeline(stages=[preprocessing_pipeline, lr])\n",
    "pipeline_rf = Pipeline(stages=[preprocessing_pipeline, rf])\n",
    "pipeline_gbt = Pipeline(stages=[preprocessing_pipeline, gbt])\n",
    "\n",
    "# 3. Latih model\n",
    "# Model dilatih pada data latih yang sudah SEIMBANG (Oversampled)\n",
    "print(\"Melatih Logistic Regression...\")\n",
    "model_lr = pipeline_lr.fit(train_data_oversampled)\n",
    "\n",
    "print(\"Melatih Random Forest...\")\n",
    "model_rf = pipeline_rf.fit(train_data_oversampled)\n",
    "\n",
    "print(\"Melatih GBT...\")\n",
    "model_gbt = pipeline_gbt.fit(train_data_oversampled)\n",
    "\n",
    "print(\"Semua model selesai dilatih.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb06e7",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "206145e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membuat prediksi pada data uji (unseen & imbalanced)...\n",
      "\n",
      "--- Hasil Evaluasi Model ---\n",
      "\n",
      "Logistic Regression:\n",
      "  AUC-ROC: 0.9239\n",
      "  AUC-PR (Fokus Churn): 0.5024\n",
      "\n",
      "Random Forest:\n",
      "  AUC-ROC: 0.9464\n",
      "  AUC-PR (Fokus Churn): 0.6996\n",
      "\n",
      "GBT Classifier:\n",
      "  AUC-ROC: 0.9679\n",
      "  AUC-PR (Fokus Churn): 0.7760\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model pada Data Test (evaluasi balik ke data yg ga seimbang u/ lihat performa nyata model)\n",
    "# 1. Buat prediksi pada data UJI (yang tidak seimbang)\n",
    "print(\"Membuat prediksi pada data uji (unseen & imbalanced)...\")\n",
    "pred_lr = model_lr.transform(test_data)\n",
    "pred_rf = model_rf.transform(test_data)\n",
    "pred_gbt = model_gbt.transform(test_data)\n",
    "\n",
    "# 2. Definisikan Evaluator\n",
    "# menggunakan dua metrik utama untuk data imbalance:\n",
    "# AUC-ROC: Baik untuk mengukur performa keseluruhan\n",
    "# AUC-PR: (AreaUnderPrecisionRecall) Sangat baik untuk kelas minoritas yang langka\n",
    "\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_churn\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_churn\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "# 3. Hitung dan Tampilkan Hasil\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Hasil Evaluasi Model ---\")\n",
    "\n",
    "# Logistic Regression\n",
    "auc_roc_lr = evaluator_roc.evaluate(pred_lr)\n",
    "auc_pr_lr = evaluator_pr.evaluate(pred_lr)\n",
    "results['Logistic Regression'] = {'AUC-ROC': auc_roc_lr, 'AUC-PR': auc_pr_lr}\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  AUC-ROC: {auc_roc_lr:.4f}\")\n",
    "print(f\"  AUC-PR (Fokus Churn): {auc_pr_lr:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "auc_roc_rf = evaluator_roc.evaluate(pred_rf)\n",
    "auc_pr_rf = evaluator_pr.evaluate(pred_rf)\n",
    "results['Random Forest'] = {'AUC-ROC': auc_roc_rf, 'AUC-PR': auc_pr_rf}\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  AUC-ROC: {auc_roc_rf:.4f}\")\n",
    "print(f\"  AUC-PR (Fokus Churn): {auc_pr_rf:.4f}\")\n",
    "\n",
    "# GBT\n",
    "auc_roc_gbt = evaluator_roc.evaluate(pred_gbt)\n",
    "auc_pr_gbt = evaluator_pr.evaluate(pred_gbt)\n",
    "results['GBT'] = {'AUC-ROC': auc_roc_gbt, 'AUC-PR': auc_pr_gbt}\n",
    "print(f\"\\nGBT Classifier:\")\n",
    "print(f\"  AUC-ROC: {auc_roc_gbt:.4f}\")\n",
    "print(f\"  AUC-PR (Fokus Churn): {auc_pr_gbt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffea7e",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46aa22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda-envs\\spark\\Lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 447.0 failed 1 times, most recent failure: Lost task 0.0 in stage 447.0 (TID 9756) (LAPTOP-HJ0CVK6G executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:189)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:258)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:279)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:189)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:189)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.IOException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:258)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:279)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  F1-Score (Churn=1):  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mF1_Churn\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Jalankan untuk model terbaik (misal, GBT)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mprint_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_gbt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGBT Classifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Jalankan untuk Logistic Regression\u001b[39;00m\n\u001b[32m     35\u001b[39m print_confusion_matrix(pred_lr, \u001b[33m\"\u001b[39m\u001b[33mLogistic Regression\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mprint_confusion_matrix\u001b[39m\u001b[34m(predictions, model_name)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_confusion_matrix\u001b[39m(predictions, model_name):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Mengubah prediksi menjadi RDD untuk MulticlassMetrics\u001b[39;00m\n\u001b[32m      6\u001b[39m     preds_and_labels = predictions.select(\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mis_churn\u001b[39m\u001b[33m\"\u001b[39m).rdd.map(\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m r: (\u001b[38;5;28mfloat\u001b[39m(r.prediction), \u001b[38;5;28mfloat\u001b[39m(r.is_churn))\n\u001b[32m      8\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     metrics = \u001b[43mMulticlassMetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_and_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     confusion_matrix = metrics.confusionMatrix().toArray()\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Confusion Matrix untuk: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\pyspark\\mllib\\evaluation.py:287\u001b[39m, in \u001b[36mMulticlassMetrics.__init__\u001b[39m\u001b[34m(self, predictionAndLabels)\u001b[39m\n\u001b[32m    285\u001b[39m sc = predictionAndLabels.ctx\n\u001b[32m    286\u001b[39m sql_ctx = SQLContext.getOrCreate(sc)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m numCol = \u001b[38;5;28mlen\u001b[39m(\u001b[43mpredictionAndLabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    288\u001b[39m schema = StructType(\n\u001b[32m    289\u001b[39m     [\n\u001b[32m    290\u001b[39m         StructField(\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m, DoubleType(), nullable=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    291\u001b[39m         StructField(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, DoubleType(), nullable=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    292\u001b[39m     ]\n\u001b[32m    293\u001b[39m )\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numCol >= \u001b[32m3\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\pyspark\\core\\rdd.py:2755\u001b[39m, in \u001b[36mRDD.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2729\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[T]\u001b[39m\u001b[33m\"\u001b[39m) -> T:\n\u001b[32m   2730\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[33;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[32m   2732\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2753\u001b[39m \u001b[33;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[32m   2754\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2755\u001b[39m     rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2756\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[32m   2757\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\pyspark\\core\\rdd.py:2722\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   2719\u001b[39m         taken += \u001b[32m1\u001b[39m\n\u001b[32m   2721\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m2722\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2724\u001b[39m items += res\n\u001b[32m   2725\u001b[39m partsScanned += numPartsToTry\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\pyspark\\core\\context.py:2551\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   2549\u001b[39m mappedRDD = rdd.mapPartitions(partitionFunc)\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\conda-envs\\spark\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 447.0 failed 1 times, most recent failure: Lost task 0.0 in stage 447.0 (TID 9756) (LAPTOP-HJ0CVK6G executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:189)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:258)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:279)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:189)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:189)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.IOException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:258)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:279)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def print_confusion_matrix(predictions, model_name):\n",
    "    # Mengubah prediksi menjadi RDD untuk MulticlassMetrics\n",
    "    preds_and_labels = predictions.select(\"prediction\", \"is_churn\").rdd.map(\n",
    "        lambda r: (float(r.prediction), float(r.is_churn))\n",
    "    )\n",
    "    \n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    print(f\"\\n--- Confusion Matrix untuk: {model_name} ---\")\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    # TN, FP\n",
    "    # FN, TP\n",
    "    TN = confusion_matrix[0][0]\n",
    "    FP = confusion_matrix[0][1]\n",
    "    FN = confusion_matrix[1][0]\n",
    "    TP = confusion_matrix[1][1]\n",
    "    \n",
    "    Recall_Churn = TP / (TP + FN)\n",
    "    Precision_Churn = TP / (TP + FP)\n",
    "    F1_Churn = 2 * (Precision_Churn * Recall_Churn) / (Precision_Churn + Recall_Churn)\n",
    "    \n",
    "    print(f\"  Recall (Churn=1):    {Recall_Churn:.4f}\")\n",
    "    print(f\"  Precision (Churn=1): {Precision_Churn:.4f}\")\n",
    "    print(f\"  F1-Score (Churn=1):  {F1_Churn:.4f}\")\n",
    "\n",
    "# Jalankan untuk model terbaik (misal, GBT)\n",
    "print_confusion_matrix(pred_gbt, \"GBT Classifier\")\n",
    "\n",
    "# Jalankan untuk Logistic Regression\n",
    "print_confusion_matrix(pred_lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5749b56",
   "metadata": {},
   "source": [
    "# Punya peli masih an cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a71c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Melatih dan Menguji: Logistic Regression ---\n",
      "\n",
      "--- Melatih dan Menguji: Decision Tree ---\n",
      "\n",
      "--- Melatih dan Menguji: Random Forest ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate \n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, f1_score\n",
    "import numpy as np \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tentukan strategi Cross-Validation (disarankan K=5 atau K=10)\n",
    "# StratifiedKFold WAJIB karena data churn tidak seimbang\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Definisikan metrik yang akan diukur (fokus pada kelas '1'/churn)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy', \n",
    "    # Metrik untuk CHURN (Kelas 1)\n",
    "    'recall_churn': make_scorer(recall_score, pos_label=1),\n",
    "    'precision_churn': make_scorer(precision_score, pos_label=1),\n",
    "    'f1_churn': make_scorer(f1_score, pos_label=1),\n",
    "\n",
    "    # Metrik untuk TIDAK CHURN (Kelas 0) \n",
    "    'recall_non_churn': make_scorer(recall_score, pos_label=0),\n",
    "    'precision_non_churn': make_scorer(precision_score, pos_label=0),\n",
    "    'f1_non_churn': make_scorer(f1_score, pos_label=0)\n",
    "}\n",
    "\n",
    "results_cv = {}    # Hasil dari cross_validate\n",
    "results_split = {} # Hasil dari single fit/predict\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Melatih dan Menguji: {model_name} ---\")\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # =======================================================\n",
    "    # BAGIAN 1: SINGLE-SPLIT (Dibutuhkan untuk melihat report_dict lengkap)\n",
    "    # =======================================================\n",
    "    full_pipeline.fit(X_train, y_train) # Latih model untuk single-split\n",
    "    y_pred = full_pipeline.predict(X_test)\n",
    "    \n",
    "    # Simpan hasil single-split (untuk visualisasi report lengkap)\n",
    "    results_split[model_name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # =======================================================\n",
    "    # BAGIAN 2: CROSS-VALIDATION (Untuk mendapatkan skor paling andal)\n",
    "    # =======================================================\n",
    "    cv_scores = cross_validate(\n",
    "        full_pipeline, \n",
    "        X, y, # Menggunakan SEMUA data sampel\n",
    "        cv=cv, \n",
    "        scoring=scoring, \n",
    "        return_train_score=False, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Rata-ratakan skor dari 5 fold\n",
    "    # Pastikan 'accuracy' sudah ditambahkan ke dictionary 'scoring' Anda\n",
    "    avg_scores = {\n",
    "        'avg_accuracy': np.mean(cv_scores['test_accuracy']),\n",
    "        \n",
    "        # CHURN (Kelas 1)\n",
    "        'avg_recall_churn': np.mean(cv_scores['test_recall_churn']),\n",
    "        'avg_precision_churn': np.mean(cv_scores['test_precision_churn']),\n",
    "        'avg_f1_churn': np.mean(cv_scores['test_f1_churn']),\n",
    "\n",
    "        # TIDAK CHURN (Kelas 0) \n",
    "        'avg_recall_non_churn': np.mean(cv_scores['test_recall_non_churn']),\n",
    "        'avg_precision_non_churn': np.mean(cv_scores['test_precision_non_churn']),\n",
    "        'avg_f1_non_churn': np.mean(cv_scores['test_f1_non_churn']),\n",
    "    }\n",
    "\n",
    "    results_cv[model_name] = avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbf8801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model: **Logistic Regression** ---\n",
      "  HASIL SPLIT TUNGGAL (80/20):\n",
      "    Akurasi Keseluruhan: 0.8173\n",
      "\n",
      "     CHURN (Kelas 1):\n",
      "      Recall:    0.9078\n",
      "      Precision: 0.3218\n",
      "      F1-Score:  0.4751\n",
      "\n",
      "     TIDAK CHURN (Kelas 0):\n",
      "      Recall: 0.8082\n",
      "      Precision: 0.9887\n",
      "      F1-Score: 0.8894\n",
      "\n",
      "  HASIL CROSS-VALIDATION (Rata-rata 5-Fold):\n",
      "    Akurasi Keseluruhan: 0.8170\n",
      "\n",
      "     CHURN (Kelas 1):\n",
      "      Recall (Churn=1):    0.9059\n",
      "      Precision (Churn=1): 0.3212\n",
      "      F1-Score (Churn=1):  0.4742\n",
      "\n",
      "     TIDAK CHURN (Kelas 0):\n",
      "      Recall (Non-Churn=0):    0.8081\n",
      "      Precision (Non-Churn=0): 0.9885\n",
      "      F1-Score (Non-Churn=0):  0.8892\n",
      "--------------------------------------------------------\n",
      "\n",
      "--- Model: **Decision Tree** ---\n",
      "  HASIL SPLIT TUNGGAL (80/20):\n",
      "    Akurasi Keseluruhan: 0.8853\n",
      "\n",
      "     CHURN (Kelas 1):\n",
      "      Recall:    0.6760\n",
      "      Precision: 0.4196\n",
      "      F1-Score:  0.5178\n",
      "\n",
      "     TIDAK CHURN (Kelas 0):\n",
      "      Recall: 0.9063\n",
      "      Precision: 0.9654\n",
      "      F1-Score: 0.9349\n",
      "\n",
      "  HASIL CROSS-VALIDATION (Rata-rata 5-Fold):\n",
      "    Akurasi Keseluruhan: 0.8849\n",
      "\n",
      "     CHURN (Kelas 1):\n",
      "      Recall (Churn=1):    0.6973\n",
      "      Precision (Churn=1): 0.4207\n",
      "      F1-Score (Churn=1):  0.5248\n",
      "\n",
      "     TIDAK CHURN (Kelas 0):\n",
      "      Recall (Non-Churn=0):    0.9037\n",
      "      Precision (Non-Churn=0): 0.9675\n",
      "      F1-Score (Non-Churn=0):  0.9345\n",
      "--------------------------------------------------------\n",
      "\n",
      "--- Model: **Random Forest** ---\n",
      "  HASIL SPLIT TUNGGAL (80/20):\n",
      "    Akurasi Keseluruhan: 0.8978\n",
      "\n",
      "     CHURN (Kelas 1):\n",
      "      Recall:    0.6349\n",
      "      Precision: 0.4561\n",
      "      F1-Score:  0.5309\n",
      "\n",
      "     TIDAK CHURN (Kelas 0):\n",
      "      Recall: 0.9241\n",
      "      Precision: 0.9619\n",
      "      F1-Score: 0.9426\n",
      "\n",
      "  HASIL CROSS-VALIDATION (Rata-rata 5-Fold):\n",
      "    Akurasi Keseluruhan: 0.8987\n",
      "\n",
      "     CHURN (Kelas 1):\n",
      "      Recall (Churn=1):    0.6471\n",
      "      Precision (Churn=1): 0.4603\n",
      "      F1-Score (Churn=1):  0.5379\n",
      "\n",
      "     TIDAK CHURN (Kelas 0):\n",
      "      Recall (Non-Churn=0):    0.9239\n",
      "      Precision (Non-Churn=0): 0.9631\n",
      "      F1-Score (Non-Churn=0):  0.9431\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for model_name in models.keys():\n",
    "    report_split = results_split.get(model_name)\n",
    "    report_cv = results_cv.get(model_name)\n",
    "    \n",
    "    if not report_split or not report_cv:\n",
    "        print(f\"\\n--- Model: {model_name} (Hasil tidak lengkap) ---\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Model: **{model_name}** ---\")\n",
    "    # --- 1. Hasil dari Single Split (fit/predict) ---\n",
    "    print(\"  HASIL SPLIT TUNGGAL (80/20):\")\n",
    "    print(f\"    Akurasi Keseluruhan: {report_split['accuracy']:.4f}\")\n",
    "    \n",
    "    if '1' in report_split:\n",
    "        print(\"\\n     CHURN (Kelas 1):\")\n",
    "        print(f\"      Recall:    {report_split['1']['recall']:.4f}\")\n",
    "        print(f\"      Precision: {report_split['1']['precision']:.4f}\")\n",
    "        print(f\"      F1-Score:  {report_split['1']['f1-score']:.4f}\")\n",
    "    if '0' in report_split:\n",
    "        print(\"\\n     TIDAK CHURN (Kelas 0):\")\n",
    "        print(f\"      Recall: {report_split['0']['recall']:.4f}\")\n",
    "        print(f\"      Precision: {report_split['0']['precision']:.4f}\")\n",
    "        print(f\"      F1-Score: {report_split['0']['f1-score']:.4f}\")\n",
    "        \n",
    "    # --- 2. Hasil dari Cross-Validation (Rata-rata) ---\n",
    "    print(\"\\n  HASIL CROSS-VALIDATION (Rata-rata 5-Fold):\")\n",
    "    print(f\"    Akurasi Keseluruhan: {report_cv['avg_accuracy']:.4f}\")\n",
    "    \n",
    "    # Hasil Rata-rata CHURN (Kelas 1)\n",
    "    print(\"\\n     CHURN (Kelas 1):\")\n",
    "    print(f\"      Recall (Churn=1):    {report_cv['avg_recall_churn']:.4f}\")\n",
    "    print(f\"      Precision (Churn=1): {report_cv['avg_precision_churn']:.4f}\")\n",
    "    print(f\"      F1-Score (Churn=1):  {report_cv['avg_f1_churn']:.4f}\")\n",
    "    \n",
    "    # Hasil Rata-rata NON-CHURN (Kelas 0) <--- TAMBAHAN BARU\n",
    "    print(\"\\n     TIDAK CHURN (Kelas 0):\")\n",
    "    print(f\"      Recall (Non-Churn=0):    {report_cv['avg_recall_non_churn']:.4f}\")\n",
    "    print(f\"      Precision (Non-Churn=0): {report_cv['avg_precision_non_churn']:.4f}\")\n",
    "    print(f\"      F1-Score (Non-Churn=0):  {report_cv['avg_f1_non_churn']:.4f}\")\n",
    "    \n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f853981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ringkasan DataFrame (Fokus pada Kedua Kelas CV) ---\n",
      "                 Model  Recall_CV (Churn=1)  Precision_CV (Churn=1)  \\\n",
      "0  Logistic Regression             0.905899                0.321181   \n",
      "1        Decision Tree             0.697324                0.420681   \n",
      "2        Random Forest             0.647106                0.460295   \n",
      "\n",
      "   Recall_CV (Non-Churn=0)  Akurasi_CV  \n",
      "0                 0.808093    0.817003  \n",
      "1                 0.903742    0.884937  \n",
      "2                 0.923946    0.898726  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ringkasan DataFrame Fokus Menggunakan CV ---\")\n",
    "comparison_data = []\n",
    "for model_name in models.keys():\n",
    "    \n",
    "    # Pengecekan data karena DataFrame akan gagal jika KeyError muncul\n",
    "    if model_name not in results_cv or 'avg_recall_non_churn' not in results_cv[model_name]:\n",
    "        continue\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Recall_CV (Churn=1)\": results_cv[model_name]['avg_recall_churn'],\n",
    "        \"Precision_CV (Churn=1)\": results_cv[model_name]['avg_precision_churn'],\n",
    "        \"Recall_CV (Non-Churn=0)\": results_cv[model_name]['avg_recall_non_churn'],\n",
    "        \"Akurasi_CV\": results_cv[model_name]['avg_accuracy'],\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sortir berdasarkan metrik yang paling penting (Recall Churn)\n",
    "print(comparison_df.sort_values(by=\"Recall_CV (Churn=1)\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
